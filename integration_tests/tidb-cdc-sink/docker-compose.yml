---
version: "3"
x-image: &image
  image: ${RW_IMAGE:-risingwavelabs/risingwave:v1.8.1}
services:
  risingwave-standalone:
    <<: *image
    command: "standalone --meta-opts=\" \
                    --listen-addr 0.0.0.0:5690 \
                    --advertise-addr 0.0.0.0:5690 \
                    --dashboard-host 0.0.0.0:5691 \
                    --prometheus-host 0.0.0.0:1250 \
                    --prometheus-endpoint http://prometheus-0:9500 \
                    --backend etcd \
                    --etcd-endpoints etcd-0:2388 \
                    --state-store hummock+minio://hummockadmin:hummockadmin@minio-0:9301/hummock001 \
                    --data-directory hummock_001 \
                    --config-path /risingwave.toml\" \
                 --compute-opts=\" \
                    --config-path /risingwave.toml \
                    --listen-addr 0.0.0.0:5688 \
                    --prometheus-listener-addr 0.0.0.0:1250 \
                    --advertise-addr 0.0.0.0:5688 \
                    --async-stack-trace verbose \
                    #--parallelism 4 \
                    #--total-memory-bytes 8589934592 \
                    --role both \
                    --meta-address http://0.0.0.0:5690\" \
                 --frontend-opts=\" \
                   --config-path /risingwave.toml \
                   --listen-addr 0.0.0.0:4566 \
                   --advertise-addr 0.0.0.0:4566 \
                   --prometheus-listener-addr 0.0.0.0:1250 \
                   --health-check-listener-addr 0.0.0.0:6786 \
                   --meta-addr http://0.0.0.0:5690\" \
                 --compactor-opts=\" \
                   --listen-addr 0.0.0.0:6660 \
                   --prometheus-listener-addr 0.0.0.0:1250 \
                   --advertise-addr 0.0.0.0:6660 \
                   --meta-address http://0.0.0.0:5690\""
    expose:
      - "6660"
      - "4566"
      - "5688"
      - "5690"
      - "1250"
      - "5691"
    ports:
      - "4566:4566"
      - "5690:5690"
      - "5691:5691"
      - "1250:1250"
    depends_on:
      - etcd-0
      - minio-0
    volumes:
      - "./risingwave.toml:/risingwave.toml"
    environment:
      RUST_BACKTRACE: "1"
      # If ENABLE_TELEMETRY is not set, telemetry will start by default
      ENABLE_TELEMETRY: ${ENABLE_TELEMETRY:-true}
    container_name: risingwave-standalone
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/6660; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5688; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/4566; exit $$?;'
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/5690; exit $$?;'
      interval: 1s
      timeout: 5s
    restart: always
    deploy:
      resources:
        limits:
          memory: 28G
        reservations:
          memory: 28G

  etcd-0:
    image: "quay.io/coreos/etcd:v3.5.10"
    command:
      - /usr/local/bin/etcd
      - "--listen-client-urls"
      - "http://0.0.0.0:2388"
      - "--advertise-client-urls"
      - "http://etcd-0:2388"
      - "--listen-peer-urls"
      - "http://0.0.0.0:2389"
      - "--initial-advertise-peer-urls"
      - "http://etcd-0:2389"
      - "--listen-metrics-urls"
      - "http://0.0.0.0:2379"
      - "--name"
      - risedev-meta
      - "--max-txn-ops"
      - "999999"
      - "--max-request-bytes"
      - "10485760"
      - "--auto-compaction-mode"
      - periodic
      - "--auto-compaction-retention"
      - 1m
      - "--snapshot-count"
      - "10000"
      - "--data-dir"
      - /etcd-data
    expose:
      - "2388"
    ports:
      - "2388:2388"
      - "2389:2389"
    depends_on: []
    volumes:
      - "etcd-0:/etcd-data"
    environment: {}
    container_name: etcd-0
    healthcheck:
      test:
        - CMD
        - etcdctl
        - --endpoints=http://localhost:2388
        - endpoint
        - health
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  grafana-0:
    image: "grafana/grafana-oss:latest"
    command: []
    expose:
      - "3001"
    ports:
      - "3001:3001"
    depends_on: []
    volumes:
      - "grafana-0:/var/lib/grafana"
      - "./grafana.ini:/etc/grafana/grafana.ini"
      - "./grafana-risedev-datasource.yml:/etc/grafana/provisioning/datasources/grafana-risedev-datasource.yml"
      - "./grafana-risedev-dashboard.yml:/etc/grafana/provisioning/dashboards/grafana-risedev-dashboard.yml"
      - "./dashboards:/dashboards"
    environment: {}
    container_name: grafana-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/3001; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  minio-0:
    image: "quay.io/minio/minio:latest"
    command:
      - server
      - "--address"
      - "0.0.0.0:9301"
      - "--console-address"
      - "0.0.0.0:9400"
      - /data
    expose:
      - "9301"
      - "9400"
    ports:
      - "9301:9301"
      - "9400:9400"
    depends_on: []
    volumes:
      - "minio-0:/data"
    entrypoint: "

      /bin/sh -c '

      set -e

      mkdir -p \"/data/hummock001\"

      /usr/bin/docker-entrypoint.sh \"$$0\" \"$$@\"

      '"
    environment:
      MINIO_CI_CD: "1"
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: "http://prometheus-0:9500"
      MINIO_ROOT_PASSWORD: hummockadmin
      MINIO_ROOT_USER: hummockadmin
      MINIO_DOMAIN: "minio-0"
    container_name: minio-0
    healthcheck:
      test:
        - CMD-SHELL
        - bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/9301; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  prometheus-0:
    image: "prom/prometheus:latest"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.listen-address=0.0.0.0:9500"
      - "--storage.tsdb.retention.time=30d"
    expose:
      - "9500"
    ports:
      - "9500:9500"
    depends_on: []
    volumes:
      - "prometheus-0:/prometheus"
      - "./prometheus.yaml:/etc/prometheus/prometheus.yml"
    environment: {}
    container_name: prometheus-0
    healthcheck:
      test:
        - CMD-SHELL
        - sh -c 'printf "GET /-/healthy HTTP/1.0\n\n" | nc localhost 9500; exit $$?;'
      interval: 1s
      timeout: 5s
      retries: 5
    restart: always

  #=================== TiDB & TiCDC components ==================
  ticdc-controller:
    image: pingcap/ticdc:v6.6.0
    container_name: ticdc-controller
    entrypoint: "/cdc cli"
    command:
      - changefeed
      - create
      - --server
      - http://ticdc-capturer0:8300
      - --sink-uri
      - "kafka://kafka:9092/ticdc_default?protocol=canal-json&kafka-version=2.4.0&partition-num=3&max-message-bytes=67108864&replication-factor=1"
      - --changefeed-id
      - "ticdc-replication-task"
      - --config
      - "/changefeed.toml"
    volumes:
      - ./config/changefeed.toml:/changefeed.toml:ro
    depends_on:
      - "pd"
      - "kafka"
      - "ticdc-capturer0"
    restart: on-failure

  ticdc-capturer0:
    image: pingcap/ticdc:v6.6.0
    entrypoint: "/cdc server"
    ports:
      - "8300:8300"
    command:
      - --addr=0.0.0.0:8300
      - --pd=http://pd:2379
      - --advertise-addr=ticdc-capturer0:8300
      - --tz=UTC
    depends_on:
      - pd
      - "tidb"
      - "kafka"
    restart: on-failure

  pd:
    image: pingcap/pd:v6.6.0
    ports:
      - "2379:2379"
    volumes:
      - ./config/pd.toml:/pd.toml:ro
    command:
      - --name=pd
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd:2379
      - --advertise-peer-urls=http://pd:2380
      - --initial-cluster=pd=http://pd:2380
      - --data-dir=/data/pd
      - --config=/pd.toml
    restart: on-failure

  tikv0:
    image: pingcap/tikv:v6.6.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - /data
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv0:20160
      - --data-dir=/data/tikv0
      - --pd=pd:2379
      - --config=/tikv.toml
    depends_on:
      - "pd"
    restart: on-failure

  tikv1:
    image: pingcap/tikv:v6.6.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - /data
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv1:20160
      - --data-dir=/data/tikv1
      - --pd=pd:2379
      - --config=/tikv.toml
    depends_on:
      - "pd"
    restart: on-failure

  tikv2:
    image: pingcap/tikv:v6.6.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - /data
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv2:20160
      - --data-dir=/data/tikv2
      - --pd=pd:2379
      - --config=/tikv.toml
    depends_on:
      - "pd"
    restart: on-failure

  tidb:
    image: pingcap/tidb:v6.6.0
    ports:
      - "4000:4000"
      - "10080:10080"
    volumes:
      - ./config/tidb.toml:/tidb.toml:ro
    command:
      - --store=tikv
      - --path=pd:2379
      - --config=/tidb.toml
      - --advertise-address=tidb
    depends_on:
      - "tikv0"
      - "tikv1"
      - "tikv2"
    restart: on-failure

  #=================== Kafka ==================

  # Adapted from https://github.com/confluentinc/demo-scene/blob/master/connect-jdbc/docker-compose.yml
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.1
    platform: linux/amd64
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.1
    platform: linux/amd64
    container_name: kafka
    depends_on:
      - "zookeeper"
    ports:
      # Exposes 9092 for external connections to the broker
      # Use kafka:29092 for connections internal on the docker network
      # See https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
      - 29092:29092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

  #===================== Others ===================
  datagen:
    build: ../datagen
    depends_on:
      - tidb
    command:
      - /bin/sh
      - -c
      - /datagen --mode twitter --qps 2 mysql --host tidb --db test --port 4000 --user root --password ""
    restart: always
    container_name: datagen

  mysql:
    image: mysql:latest
    command: tail -f /dev/null
    volumes:
      - "./tidb_create_tables.sql:/tidb_create_tables.sql"
      - "./tidb_prepare.sql:/tidb_prepare.sql"
    container_name: mysql
    restart: on-failure

volumes:
  risingwave-standalone:
    external: false
  etcd-0:
    external: false
  grafana-0:
    external: false
  minio-0:
    external: false
  prometheus-0:
    external: false
  message_queue:
    external: false
name: risingwave-compose
